{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NVIDIA GeForce GTX 1060 6GB\n",
    "- Pytorch 1.4.0\n",
    "- model EfficientNet-B3\n",
    "- image size 128x128\n",
    "- batch size 64\n",
    "- 5-folds\n",
    "- 10 epochs\n",
    "- no augmentation\n",
    "- optimizer over 9000\n",
    "- one cycle learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import json\n",
    "#import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import six\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "#from typing import List\n",
    "import torch\n",
    "#from torch.nn import init\n",
    "#from torch.nn.parameter import Parameter\n",
    "#import torch.nn.functional as F\n",
    "#from torch.nn import Sequential\n",
    "#import pretrainedmodels\n",
    "import warnings\n",
    "\n",
    "from crop_resize import Resize, read_feathers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "#debug = False\n",
    "#submission = False\n",
    "batch_size = 64\n",
    "#device = 'cuda:0'\n",
    "#out = '.'\n",
    "#arch = 'pretrained'\n",
    "#model_name = 'se_resnext50_32x4d'\n",
    "model_name = 'efficientnet-b3'\n",
    "#indices=[0, 1, 2, 3]\n",
    "train_size = 0.8\n",
    "random_state = 2020\n",
    "n_epochs = 10\n",
    "\n",
    "in_dir = Path('../input/bengaliai-cv19')\n",
    "feather_dir = Path('../input/bengaliai-cv19-feather')\n",
    "out_dir = Path('./20200307_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"images\"></a>\n",
    "# Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200840, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "filenames = [feather_dir/f'train_image_data_1x{image_size}x{image_size}_{i}.feather' for i in range(4)]\n",
    "images = read_feathers(filenames, image_size)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"labels\"></a>\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200840, 3)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(in_dir/'train.csv')\n",
    "labels = train[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']].values\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataset\"></a>\n",
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GraphemeDataset(Dataset):\n",
    "    def __init__(self, images, labels=None, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.train = labels is not None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.train:\n",
    "            label = self.labels[idx]\n",
    "            return image, label[0], label[1], label[2]\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images = images[:10]\n",
    "#labels = labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images split\n",
      "(160672, 128, 128)\n",
      "(40168, 128, 128)\n",
      "labels split\n",
      "(160672, 3)\n",
      "(40168, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_images, valid_images, train_labels, valid_labels = train_test_split(images, labels, train_size=train_size,\n",
    "                                                                          random_state=random_state, shuffle=True)\n",
    "\n",
    "print('images split')\n",
    "print(train_images.shape)\n",
    "print(valid_images.shape)\n",
    "\n",
    "print('labels split')\n",
    "print(train_labels.shape)\n",
    "print(valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GraphemeDataset(train_images, train_labels)\n",
    "valid_dataset = GraphemeDataset(valid_images, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_images, train_labels, valid_images, valid_labels\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataloader\"></a>\n",
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loader length 2511\n",
      "valid loader length 628\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('train loader length', len(train_loader))\n",
    "print('valid loader length', len(valid_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a> \n",
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "missing keys :  ['_fc1.weight', '_fc1.bias', '_fc2.weight', '_fc2.bias', '_fc3.weight', '_fc3.bias']\n",
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    }
   ],
   "source": [
    "from my_efficientnet_pytorch import EfficientNet\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = EfficientNet.from_pretrained(model_name, in_channels=1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"optimizer\"></a> \n",
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer.ralamb import Ralamb\n",
    "from optimizer.lookahead import Lookahead\n",
    "\n",
    "#def LookaheadAdam(params, alpha=0.5, k=6, *args, **kwargs):\n",
    "#     adam = Adam(params, *args, **kwargs)\n",
    "#     return Lookahead(adam, alpha, k)\n",
    "\n",
    "def Over9000(params, alpha=0.5, k=6, *args, **kwargs):\n",
    "     ralamb = Ralamb(params, *args, **kwargs)\n",
    "     return Lookahead(ralamb, alpha, k)\n",
    "\n",
    "#RangerLars = Over9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer =Over9000(model.parameters(), lr=2e-3, weight_decay=1e-3)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-2, total_steps=None, epochs=n_epochs, \n",
    "                                                steps_per_epoch=len(train_loader), pct_start=0.0, anneal_strategy='cos', \n",
    "                                                cycle_momentum=True, base_momentum=0.85, max_momentum=0.95,  div_factor=100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"loss\"></a> \n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"metrics\"></a> \n",
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import macro_recall_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a> \n",
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.DataFrame()\n",
    "best_valid_recall = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa42b033f02f4a9d84e9a6b515449608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naoya\\Anaconda3\\envs\\fastai-v1\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train epoch  : 0\n",
      "      loss   : 0.7165\n",
      "      acc    : 0.8461\n",
      "      recall : 0.7504\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1155a07b7f084e5c9558dfbc40961dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "valid epoch  : 0\n",
      "      loss   : 1.4806\n",
      "      acc    : 0.9107\n",
      "      recall : 0.8378\n",
      "validation recall has increased from: 0.0000 to: 0.8378. Saving checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8829476455974ee2913605abc3f6fa9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train epoch  : 1\n",
      "      loss   : 0.2952\n",
      "      acc    : 0.9355\n",
      "      recall : 0.8799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e741dd4ef2746cdb74d013291dd7c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "valid epoch  : 1\n",
      "      loss   : 0.8157\n",
      "      acc    : 0.9504\n",
      "      recall : 0.9041\n",
      "validation recall has increased from: 0.8378 to: 0.9041. Saving checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad62eb76b2424fda99c588cf4f8e7fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train epoch  : 2\n",
      "      loss   : 0.2156\n",
      "      acc    : 0.9523\n",
      "      recall : 0.9097\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1964da699e94350a15fb1344b604dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "valid epoch  : 2\n",
      "      loss   : 0.9416\n",
      "      acc    : 0.9434\n",
      "      recall : 0.8967\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9437adc495c3419680871ca7252d5897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train epoch  : 3\n",
      "      loss   : 0.1588\n",
      "      acc    : 0.9635\n",
      "      recall : 0.9306\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16de096a36a14d17bc9d6fab5825a88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "valid epoch  : 3\n",
      "      loss   : 0.6189\n",
      "      acc    : 0.9639\n",
      "      recall : 0.9289\n",
      "validation recall has increased from: 0.9041 to: 0.9289. Saving checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c3f972814c4227b65654b8e0929fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train epoch  : 4\n",
      "      loss   : 0.1133\n",
      "      acc    : 0.9730\n",
      "      recall : 0.9488\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d01f45a40547d19845f1bc2f8c4265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "valid epoch  : 4\n",
      "      loss   : 0.5436\n",
      "      acc    : 0.9698\n",
      "      recall : 0.9398\n",
      "validation recall has increased from: 0.9289 to: 0.9398. Saving checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d472501e964f568dabdd3e58221d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train epoch  : 5\n",
      "      loss   : 0.0746\n",
      "      acc    : 0.9810\n",
      "      recall : 0.9645\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bac4721eda4f1d8f731779704fce2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "valid epoch  : 5\n",
      "      loss   : 0.4920\n",
      "      acc    : 0.9734\n",
      "      recall : 0.9472\n",
      "validation recall has increased from: 0.9398 to: 0.9472. Saving checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e334cd30e3294d7dac3193023926b51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train epoch  : 6\n",
      "      loss   : 0.0437\n",
      "      acc    : 0.9874\n",
      "      recall : 0.9772\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6641ac75304547851bfa3b2e9d61a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "valid epoch  : 6\n",
      "      loss   : 0.5049\n",
      "      acc    : 0.9745\n",
      "      recall : 0.9481\n",
      "validation recall has increased from: 0.9472 to: 0.9481. Saving checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cd8cdc784b4d3a81c665420b45cbbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train epoch  : 7\n",
      "      loss   : 0.0239\n",
      "      acc    : 0.9922\n",
      "      recall : 0.9870\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377ad26e03644c1a9335a0445ca344a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "valid epoch  : 7\n",
      "      loss   : 0.5136\n",
      "      acc    : 0.9762\n",
      "      recall : 0.9507\n",
      "validation recall has increased from: 0.9481 to: 0.9507. Saving checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6092d9578024e69873402a591f83878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train epoch  : 8\n",
      "      loss   : 0.0129\n",
      "      acc    : 0.9952\n",
      "      recall : 0.9928\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8223cb0283a84425ab3be153494d9397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "valid epoch  : 8\n",
      "      loss   : 0.5260\n",
      "      acc    : 0.9770\n",
      "      recall : 0.9528\n",
      "validation recall has increased from: 0.9507 to: 0.9528. Saving checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0faa0a71b4894c9e862ba69ff6419c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2511.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train epoch  : 9\n",
      "      loss   : 0.0092\n",
      "      acc    : 0.9965\n",
      "      recall : 0.9953\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f496dccea24f4e0bbc5eff73c718d8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "valid epoch  : 9\n",
      "      loss   : 0.5299\n",
      "      acc    : 0.9771\n",
      "      recall : 0.9529\n",
      "validation recall has increased from: 0.9528 to: 0.9529. Saving checkpoint\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # --- training start ---\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    running_loss, running_acc, running_recall = 0.0, 0.0, 0.0\n",
    "    train_loss, train_acc, train_recall = 0.0, 0.0, 0.0\n",
    "    model.train()\n",
    "    \n",
    "    # training loop\n",
    "    for idx, (inputs, labels1, labels2, labels3) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        \n",
    "        # to GPU\n",
    "        inputs, labels1, labels2, labels3 = inputs.to(device), labels1.to(device), labels2.to(device), labels3.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        outputs1, outputs2, outputs3 = model(inputs.unsqueeze(1).float())\n",
    "        loss1 = 0.7 * criterion(outputs1, labels1)\n",
    "        loss2 = 0.1 * criterion(outputs2, labels2)\n",
    "        loss3 = 0.2 * criterion(outputs3, labels3)\n",
    "        running_loss += loss1.item() + loss2.item() + loss3.item()\n",
    "        running_acc += (outputs1.argmax(1)==labels1).float().mean()\n",
    "        running_acc += (outputs2.argmax(1)==labels2).float().mean()\n",
    "        running_acc += (outputs3.argmax(1)==labels3).float().mean()\n",
    "        running_recall += macro_recall_multi(outputs1, labels1, outputs2, labels2, outputs3, labels3)\n",
    "        \n",
    "        # backward\n",
    "        (loss1 + loss2 + loss3).backward()\n",
    "        \n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = running_acc / (len(train_loader) * 3)\n",
    "    train_recall = running_recall / len(train_loader)\n",
    "    \n",
    "    print('train epoch  : {}'.format(epoch))\n",
    "    print('      loss   : {:.4f}'.format(train_loss))\n",
    "    print('      acc    : {:.4f}'.format(train_acc))\n",
    "    print('      recall : {:.4f}'.format(train_recall))\n",
    "\n",
    "    history.loc[epoch, 'train_loss'] = train_loss\n",
    "    history.loc[epoch, 'train_acc'] = train_acc.cpu().numpy()\n",
    "    history.loc[epoch, 'train_recall'] = train_recall\n",
    "    \n",
    "    # --- validation start ---\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    running_loss, running_acc, running_recall = 0.0, 0.0, 0.0\n",
    "    valid_loss, valid_acc, valid_recall = 0.0, 0.0, 0.0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for idx, (inputs, labels1, labels2, labels3) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
    "            \n",
    "            # to GPU\n",
    "            inputs, labels1, labels2, labels3 = inputs.to(device), labels1.to(device), labels2.to(device), labels3.to(device)\n",
    "            \n",
    "            # forward\n",
    "            outputs1, outputs2, outputs3 = model(inputs.unsqueeze(1).float())\n",
    "            loss1 = 2.0 * criterion(outputs1, labels1)\n",
    "            loss2 = 1.0 * criterion(outputs2, labels2)\n",
    "            loss3 = 1.0 * criterion(outputs3, labels3)\n",
    "            running_loss += loss1.item() + loss2.item() + loss3.item()\n",
    "            running_acc += (outputs1.argmax(1)==labels1).float().mean()\n",
    "            running_acc += (outputs2.argmax(1)==labels2).float().mean()\n",
    "            running_acc += (outputs3.argmax(1)==labels3).float().mean()\n",
    "            running_recall += macro_recall_multi(outputs1, labels1, outputs2, labels2, outputs3, labels3)\n",
    "            \n",
    "    valid_loss = running_loss / len(valid_loader)\n",
    "    valid_acc = running_acc / (len(valid_loader) * 3)\n",
    "    valid_recall = running_recall / len(valid_loader)\n",
    "    \n",
    "    print('valid epoch  : {}'.format(epoch))\n",
    "    print('      loss   : {:.4f}'.format(valid_loss))\n",
    "    print('      acc    : {:.4f}'.format(valid_acc))\n",
    "    print('      recall : {:.4f}'.format(valid_recall))\n",
    "    \n",
    "    history.loc[epoch, 'valid_loss'] = valid_loss\n",
    "    history.loc[epoch, 'valid_acc'] = valid_acc.cpu().numpy()\n",
    "    history.loc[epoch, 'valid_recall'] = valid_recall\n",
    "    \n",
    "    if valid_recall > best_valid_recall:\n",
    "        print(f'validation recall has increased from: {best_valid_recall:.4f} to: {valid_recall:.4f}. Saving checkpoint')\n",
    "        torch.save(model.state_dict(), out_dir/f'efficientnet-b0_{epoch}.pth')\n",
    "        best_valid_recall = valid_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_acc</th>\n",
       "      <th>valid_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.716531</td>\n",
       "      <td>0.846125</td>\n",
       "      <td>0.750375</td>\n",
       "      <td>1.480586</td>\n",
       "      <td>0.910677</td>\n",
       "      <td>0.837821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.295248</td>\n",
       "      <td>0.935546</td>\n",
       "      <td>0.879895</td>\n",
       "      <td>0.815661</td>\n",
       "      <td>0.950365</td>\n",
       "      <td>0.904117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.215565</td>\n",
       "      <td>0.952308</td>\n",
       "      <td>0.909651</td>\n",
       "      <td>0.941641</td>\n",
       "      <td>0.943445</td>\n",
       "      <td>0.896709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.158839</td>\n",
       "      <td>0.963496</td>\n",
       "      <td>0.930600</td>\n",
       "      <td>0.618874</td>\n",
       "      <td>0.963859</td>\n",
       "      <td>0.928866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.113270</td>\n",
       "      <td>0.972967</td>\n",
       "      <td>0.948769</td>\n",
       "      <td>0.543620</td>\n",
       "      <td>0.969775</td>\n",
       "      <td>0.939794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.074646</td>\n",
       "      <td>0.980994</td>\n",
       "      <td>0.964523</td>\n",
       "      <td>0.491962</td>\n",
       "      <td>0.973413</td>\n",
       "      <td>0.947245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.043740</td>\n",
       "      <td>0.987437</td>\n",
       "      <td>0.977232</td>\n",
       "      <td>0.504865</td>\n",
       "      <td>0.974458</td>\n",
       "      <td>0.948061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.023931</td>\n",
       "      <td>0.992226</td>\n",
       "      <td>0.987014</td>\n",
       "      <td>0.513597</td>\n",
       "      <td>0.976183</td>\n",
       "      <td>0.950704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.012878</td>\n",
       "      <td>0.995238</td>\n",
       "      <td>0.992752</td>\n",
       "      <td>0.525973</td>\n",
       "      <td>0.976962</td>\n",
       "      <td>0.952753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.009215</td>\n",
       "      <td>0.996466</td>\n",
       "      <td>0.995267</td>\n",
       "      <td>0.529881</td>\n",
       "      <td>0.977087</td>\n",
       "      <td>0.952942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_loss  train_acc  train_recall  valid_loss  valid_acc  valid_recall\n",
       "0    0.716531   0.846125      0.750375    1.480586   0.910677      0.837821\n",
       "1    0.295248   0.935546      0.879895    0.815661   0.950365      0.904117\n",
       "2    0.215565   0.952308      0.909651    0.941641   0.943445      0.896709\n",
       "3    0.158839   0.963496      0.930600    0.618874   0.963859      0.928866\n",
       "4    0.113270   0.972967      0.948769    0.543620   0.969775      0.939794\n",
       "5    0.074646   0.980994      0.964523    0.491962   0.973413      0.947245\n",
       "6    0.043740   0.987437      0.977232    0.504865   0.974458      0.948061\n",
       "7    0.023931   0.992226      0.987014    0.513597   0.976183      0.950704\n",
       "8    0.012878   0.995238      0.992752    0.525973   0.976962      0.952753\n",
       "9    0.009215   0.996466      0.995267    0.529881   0.977087      0.952942"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.to_csv(out_dir/'history.csv')\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"inference\"></a> \n",
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('../input/bengaliai-cv19/test_image_data_0.parquet'),\n",
       " WindowsPath('../input/bengaliai-cv19/test_image_data_1.parquet'),\n",
       " WindowsPath('../input/bengaliai-cv19/test_image_data_2.parquet'),\n",
       " WindowsPath('../input/bengaliai-cv19/test_image_data_3.parquet')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from crop_resize import read_parquets\n",
    "\n",
    "height = 137\n",
    "width = 236\n",
    "image_size = 128\n",
    "\n",
    "filenames = [in_dir/f'test_image_data_{i}.parquet' for i in range(4)]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 2999.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1481.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1497.07it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1500.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12, 128, 128)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = read_parquets(filenames, width, height, image_size)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "test_dataset = GraphemeDataset(images)\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d5e045e0c0442f959c31b889c1851c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_0_grapheme_root</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_0_vowel_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_0_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_1_grapheme_root</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_1_vowel_diacritic</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Test_1_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Test_2_grapheme_root</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Test_2_vowel_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Test_2_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Test_3_grapheme_root</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Test_3_vowel_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Test_3_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Test_4_grapheme_root</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Test_4_vowel_diacritic</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Test_4_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Test_5_grapheme_root</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Test_5_vowel_diacritic</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Test_5_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Test_6_grapheme_root</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Test_6_vowel_diacritic</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        row_id  target\n",
       "0         Test_0_grapheme_root       3\n",
       "1       Test_0_vowel_diacritic       0\n",
       "2   Test_0_consonant_diacritic       0\n",
       "3         Test_1_grapheme_root      93\n",
       "4       Test_1_vowel_diacritic       2\n",
       "5   Test_1_consonant_diacritic       0\n",
       "6         Test_2_grapheme_root      19\n",
       "7       Test_2_vowel_diacritic       0\n",
       "8   Test_2_consonant_diacritic       0\n",
       "9         Test_3_grapheme_root     115\n",
       "10      Test_3_vowel_diacritic       0\n",
       "11  Test_3_consonant_diacritic       0\n",
       "12        Test_4_grapheme_root      55\n",
       "13      Test_4_vowel_diacritic       4\n",
       "14  Test_4_consonant_diacritic       0\n",
       "15        Test_5_grapheme_root     115\n",
       "16      Test_5_vowel_diacritic       2\n",
       "17  Test_5_consonant_diacritic       0\n",
       "18        Test_6_grapheme_root     147\n",
       "19      Test_6_vowel_diacritic       9"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_id = []\n",
    "target = []\n",
    "\n",
    "for idx, inputs in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "    outputs1, outputs2, outputs3 = model(inputs.unsqueeze(1).float().cuda())\n",
    "    p1 = outputs1.argmax(-1).view(-1).cpu()\n",
    "    p2 = outputs2.argmax(-1).view(-1).cpu()\n",
    "    p3 = outputs3.argmax(-1).view(-1).cpu()\n",
    "    row_id += [f'Test_{idx}_grapheme_root', f'Test_{idx}_vowel_diacritic', f'Test_{idx}_consonant_diacritic']\n",
    "    target += [p1.item(),p2.item(),p3.item()]\n",
    "\n",
    "sub_df = pd.DataFrame({'row_id': row_id, 'target': target})\n",
    "sub_df.to_csv(out_dir/'submission.csv', index=False)\n",
    "sub_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # --- training start ---\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    running_loss, running_acc, running_recall = 0.0, 0.0, 0.0\n",
    "    train_loss, train_acc, train_recall = 0.0, 0.0, 0.0\n",
    "    model.train()\n",
    "    \n",
    "    # training loop\n",
    "    for idx, (inputs, labels1, labels2, labels3) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        \n",
    "        # to GPU\n",
    "        inputs, labels1, labels2, labels3 = inputs.to(device), labels1.to(device), labels2.to(device), labels3.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        outputs1, outputs2, outputs3 = model(inputs.unsqueeze(1).float())\n",
    "        loss1 = 0.7 * criterion(outputs1, labels1)\n",
    "        loss2 = 0.1 * criterion(outputs2, labels2)\n",
    "        loss3 = 0.2 * criterion(outputs3, labels3)\n",
    "        running_loss += loss1.item() + loss2.item() + loss3.item()\n",
    "        running_acc += (outputs1.argmax(1)==labels1).float().mean()\n",
    "        running_acc += (outputs2.argmax(1)==labels2).float().mean()\n",
    "        running_acc += (outputs3.argmax(1)==labels3).float().mean()\n",
    "        running_recall += macro_recall_multi(outputs1, labels1, outputs2, labels2, outputs3, labels3)\n",
    "        \n",
    "        # backward\n",
    "        (loss1 + loss2 + loss3).backward()\n",
    "        \n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = running_acc / (len(train_loader) * 3)\n",
    "    train_recall = running_recall / len(train_loader)\n",
    "    \n",
    "    print('train epoch  : {}'.format(epoch))\n",
    "    print('      loss   : {:.4f}'.format(train_loss))\n",
    "    print('      acc    : {:.4f}'.format(train_acc))\n",
    "    print('      recall : {:.4f}'.format(train_recall))\n",
    "\n",
    "    history.loc[epoch, 'train_loss'] = train_loss\n",
    "    history.loc[epoch, 'train_acc'] = train_acc.cpu().numpy()\n",
    "    history.loc[epoch, 'train_recall'] = train_recall\n",
    "    \n",
    "    torch.save(model.state_dict(), out_dir/f'efficientnet-b0_{epoch}.pth')\n",
    "    \n",
    "    # --- validation start ---\n",
    "    if train_size < 1.0:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        running_loss, running_acc, running_recall = 0.0, 0.0, 0.0\n",
    "        valid_loss, valid_acc, valid_recall = 0.0, 0.0, 0.0\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for idx, (inputs, labels1, labels2, labels3) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
    "                \n",
    "                # to GPU\n",
    "                inputs, labels1, labels2, labels3 = inputs.to(device), labels1.to(device), labels2.to(device), labels3.to(device)\n",
    "                \n",
    "                # forward\n",
    "                outputs1, outputs2, outputs3 = model(inputs.unsqueeze(1).float())\n",
    "                loss1 = 0.50 * criterion(outputs1, labels1)\n",
    "                loss2 = 0.25 * criterion(outputs2, labels2)\n",
    "                loss3 = 0.25 * criterion(outputs3, labels3)\n",
    "                running_loss += loss1.item() + loss2.item() + loss3.item()\n",
    "                running_acc += (outputs1.argmax(1)==labels1).float().mean()\n",
    "                running_acc += (outputs2.argmax(1)==labels2).float().mean()\n",
    "                running_acc += (outputs3.argmax(1)==labels3).float().mean()\n",
    "                running_recall += macro_recall_multi(outputs1, labels1, outputs2, labels2, outputs3, labels3)\n",
    "                \n",
    "        valid_loss = running_loss / len(valid_loader)\n",
    "        valid_acc = running_acc / (len(valid_loader) * 3)\n",
    "        valid_recall = running_recall / len(valid_loader)\n",
    "        \n",
    "        print('valid epoch  : {}'.format(epoch))\n",
    "        print('      loss   : {:.4f}'.format(valid_loss))\n",
    "        print('      acc    : {:.4f}'.format(valid_acc))\n",
    "        print('      recall : {:.4f}'.format(valid_recall))\n",
    "        \n",
    "        history.loc[epoch, 'valid_loss'] = valid_loss\n",
    "        history.loc[epoch, 'valid_acc'] = valid_acc.cpu().numpy()\n",
    "        history.loc[epoch, 'valid_recall'] = valid_recall\n",
    "        \n",
    "        if valid_recall > best_valid_recall:\n",
    "            print(f'validation recall has increased from: {best_valid_recall:.4f} to: {valid_recall:.4f}. Saving checkpoint')\n",
    "            torch.save(model.state_dict(), out_dir/f'{model_name}_{epoch}.pth')\n",
    "            best_valid_recall = valid_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>iteration</th>\n",
       "      <th>train/loss</th>\n",
       "      <th>train/loss_grapheme</th>\n",
       "      <th>train/loss_vowel</th>\n",
       "      <th>train/loss_consonant</th>\n",
       "      <th>train/acc_grapheme</th>\n",
       "      <th>train/acc_vowel</th>\n",
       "      <th>train/acc_consonant</th>\n",
       "      <th>train/recall</th>\n",
       "      <th>valid/loss</th>\n",
       "      <th>valid/loss_grapheme</th>\n",
       "      <th>valid/loss_vowel</th>\n",
       "      <th>valid/loss_consonant</th>\n",
       "      <th>valid/acc_grapheme</th>\n",
       "      <th>valid/acc_vowel</th>\n",
       "      <th>valid/acc_consonant</th>\n",
       "      <th>valid/recall</th>\n",
       "      <th>lr</th>\n",
       "      <th>elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1883</td>\n",
       "      <td>3.042697</td>\n",
       "      <td>1.797872</td>\n",
       "      <td>0.632122</td>\n",
       "      <td>0.612704</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.824641</td>\n",
       "      <td>0.851282</td>\n",
       "      <td>0.677516</td>\n",
       "      <td>0.848359</td>\n",
       "      <td>0.480272</td>\n",
       "      <td>0.173948</td>\n",
       "      <td>0.194139</td>\n",
       "      <td>0.875774</td>\n",
       "      <td>0.960387</td>\n",
       "      <td>0.960526</td>\n",
       "      <td>0.909214</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2243.661856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3766</td>\n",
       "      <td>1.158309</td>\n",
       "      <td>0.645604</td>\n",
       "      <td>0.287593</td>\n",
       "      <td>0.225112</td>\n",
       "      <td>0.837768</td>\n",
       "      <td>0.912725</td>\n",
       "      <td>0.938502</td>\n",
       "      <td>0.873624</td>\n",
       "      <td>0.695029</td>\n",
       "      <td>0.378965</td>\n",
       "      <td>0.182676</td>\n",
       "      <td>0.133387</td>\n",
       "      <td>0.896022</td>\n",
       "      <td>0.942252</td>\n",
       "      <td>0.966488</td>\n",
       "      <td>0.916853</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4485.461718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5649</td>\n",
       "      <td>0.907438</td>\n",
       "      <td>0.502178</td>\n",
       "      <td>0.232452</td>\n",
       "      <td>0.172807</td>\n",
       "      <td>0.871242</td>\n",
       "      <td>0.927178</td>\n",
       "      <td>0.948852</td>\n",
       "      <td>0.898889</td>\n",
       "      <td>0.494653</td>\n",
       "      <td>0.275289</td>\n",
       "      <td>0.122158</td>\n",
       "      <td>0.097206</td>\n",
       "      <td>0.927242</td>\n",
       "      <td>0.966032</td>\n",
       "      <td>0.973413</td>\n",
       "      <td>0.939164</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6727.550425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7532</td>\n",
       "      <td>0.787359</td>\n",
       "      <td>0.430356</td>\n",
       "      <td>0.206906</td>\n",
       "      <td>0.150097</td>\n",
       "      <td>0.887900</td>\n",
       "      <td>0.933570</td>\n",
       "      <td>0.953656</td>\n",
       "      <td>0.912072</td>\n",
       "      <td>0.408498</td>\n",
       "      <td>0.241828</td>\n",
       "      <td>0.092366</td>\n",
       "      <td>0.074304</td>\n",
       "      <td>0.935774</td>\n",
       "      <td>0.976637</td>\n",
       "      <td>0.979911</td>\n",
       "      <td>0.946010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>8968.730490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>9415</td>\n",
       "      <td>0.699629</td>\n",
       "      <td>0.377690</td>\n",
       "      <td>0.187407</td>\n",
       "      <td>0.134532</td>\n",
       "      <td>0.900114</td>\n",
       "      <td>0.939088</td>\n",
       "      <td>0.958386</td>\n",
       "      <td>0.920294</td>\n",
       "      <td>0.412999</td>\n",
       "      <td>0.236399</td>\n",
       "      <td>0.102951</td>\n",
       "      <td>0.073649</td>\n",
       "      <td>0.938056</td>\n",
       "      <td>0.972619</td>\n",
       "      <td>0.980060</td>\n",
       "      <td>0.951533</td>\n",
       "      <td>0.001</td>\n",
       "      <td>11209.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>11298</td>\n",
       "      <td>0.641063</td>\n",
       "      <td>0.340617</td>\n",
       "      <td>0.176144</td>\n",
       "      <td>0.124302</td>\n",
       "      <td>0.909256</td>\n",
       "      <td>0.941613</td>\n",
       "      <td>0.960942</td>\n",
       "      <td>0.927734</td>\n",
       "      <td>0.409563</td>\n",
       "      <td>0.246129</td>\n",
       "      <td>0.092293</td>\n",
       "      <td>0.071141</td>\n",
       "      <td>0.934236</td>\n",
       "      <td>0.976091</td>\n",
       "      <td>0.979673</td>\n",
       "      <td>0.946735</td>\n",
       "      <td>0.001</td>\n",
       "      <td>13449.830865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>13181</td>\n",
       "      <td>0.588314</td>\n",
       "      <td>0.309447</td>\n",
       "      <td>0.164080</td>\n",
       "      <td>0.114787</td>\n",
       "      <td>0.916313</td>\n",
       "      <td>0.945563</td>\n",
       "      <td>0.964307</td>\n",
       "      <td>0.933129</td>\n",
       "      <td>0.350197</td>\n",
       "      <td>0.194251</td>\n",
       "      <td>0.096681</td>\n",
       "      <td>0.059265</td>\n",
       "      <td>0.947093</td>\n",
       "      <td>0.972679</td>\n",
       "      <td>0.984028</td>\n",
       "      <td>0.950324</td>\n",
       "      <td>0.001</td>\n",
       "      <td>15688.977573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>15064</td>\n",
       "      <td>0.552341</td>\n",
       "      <td>0.288925</td>\n",
       "      <td>0.155283</td>\n",
       "      <td>0.108134</td>\n",
       "      <td>0.921472</td>\n",
       "      <td>0.948023</td>\n",
       "      <td>0.966103</td>\n",
       "      <td>0.937743</td>\n",
       "      <td>0.332787</td>\n",
       "      <td>0.198453</td>\n",
       "      <td>0.075235</td>\n",
       "      <td>0.059099</td>\n",
       "      <td>0.946687</td>\n",
       "      <td>0.979911</td>\n",
       "      <td>0.982748</td>\n",
       "      <td>0.955584</td>\n",
       "      <td>0.001</td>\n",
       "      <td>17927.412009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>16947</td>\n",
       "      <td>0.523410</td>\n",
       "      <td>0.270066</td>\n",
       "      <td>0.149229</td>\n",
       "      <td>0.104115</td>\n",
       "      <td>0.925769</td>\n",
       "      <td>0.949912</td>\n",
       "      <td>0.967451</td>\n",
       "      <td>0.940794</td>\n",
       "      <td>0.338975</td>\n",
       "      <td>0.199955</td>\n",
       "      <td>0.080003</td>\n",
       "      <td>0.059017</td>\n",
       "      <td>0.945744</td>\n",
       "      <td>0.978571</td>\n",
       "      <td>0.982540</td>\n",
       "      <td>0.958276</td>\n",
       "      <td>0.001</td>\n",
       "      <td>20167.192780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>18830</td>\n",
       "      <td>0.491826</td>\n",
       "      <td>0.250700</td>\n",
       "      <td>0.143343</td>\n",
       "      <td>0.097783</td>\n",
       "      <td>0.930137</td>\n",
       "      <td>0.951868</td>\n",
       "      <td>0.968854</td>\n",
       "      <td>0.944074</td>\n",
       "      <td>0.317771</td>\n",
       "      <td>0.193612</td>\n",
       "      <td>0.070374</td>\n",
       "      <td>0.053785</td>\n",
       "      <td>0.948274</td>\n",
       "      <td>0.981300</td>\n",
       "      <td>0.985972</td>\n",
       "      <td>0.957762</td>\n",
       "      <td>0.001</td>\n",
       "      <td>22406.465798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>20713</td>\n",
       "      <td>0.470240</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>0.137160</td>\n",
       "      <td>0.094564</td>\n",
       "      <td>0.933490</td>\n",
       "      <td>0.953323</td>\n",
       "      <td>0.970448</td>\n",
       "      <td>0.946249</td>\n",
       "      <td>0.310357</td>\n",
       "      <td>0.178886</td>\n",
       "      <td>0.075267</td>\n",
       "      <td>0.056204</td>\n",
       "      <td>0.953175</td>\n",
       "      <td>0.980952</td>\n",
       "      <td>0.984673</td>\n",
       "      <td>0.960563</td>\n",
       "      <td>0.001</td>\n",
       "      <td>24645.514050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>22596</td>\n",
       "      <td>0.448688</td>\n",
       "      <td>0.226095</td>\n",
       "      <td>0.131934</td>\n",
       "      <td>0.090659</td>\n",
       "      <td>0.936000</td>\n",
       "      <td>0.955284</td>\n",
       "      <td>0.971813</td>\n",
       "      <td>0.948729</td>\n",
       "      <td>0.307876</td>\n",
       "      <td>0.175234</td>\n",
       "      <td>0.078108</td>\n",
       "      <td>0.054535</td>\n",
       "      <td>0.953274</td>\n",
       "      <td>0.980159</td>\n",
       "      <td>0.985218</td>\n",
       "      <td>0.961931</td>\n",
       "      <td>0.001</td>\n",
       "      <td>26882.418484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>24479</td>\n",
       "      <td>0.432559</td>\n",
       "      <td>0.214815</td>\n",
       "      <td>0.129309</td>\n",
       "      <td>0.088435</td>\n",
       "      <td>0.939328</td>\n",
       "      <td>0.955757</td>\n",
       "      <td>0.971614</td>\n",
       "      <td>0.950347</td>\n",
       "      <td>0.279739</td>\n",
       "      <td>0.161382</td>\n",
       "      <td>0.067744</td>\n",
       "      <td>0.050614</td>\n",
       "      <td>0.957202</td>\n",
       "      <td>0.983829</td>\n",
       "      <td>0.985565</td>\n",
       "      <td>0.965432</td>\n",
       "      <td>0.001</td>\n",
       "      <td>29116.622837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>26362</td>\n",
       "      <td>0.417307</td>\n",
       "      <td>0.207002</td>\n",
       "      <td>0.125265</td>\n",
       "      <td>0.085039</td>\n",
       "      <td>0.941782</td>\n",
       "      <td>0.957059</td>\n",
       "      <td>0.972991</td>\n",
       "      <td>0.952580</td>\n",
       "      <td>0.284997</td>\n",
       "      <td>0.165484</td>\n",
       "      <td>0.069119</td>\n",
       "      <td>0.050394</td>\n",
       "      <td>0.956399</td>\n",
       "      <td>0.982391</td>\n",
       "      <td>0.985923</td>\n",
       "      <td>0.963083</td>\n",
       "      <td>0.001</td>\n",
       "      <td>31354.389972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>28245</td>\n",
       "      <td>0.399159</td>\n",
       "      <td>0.196582</td>\n",
       "      <td>0.121286</td>\n",
       "      <td>0.081291</td>\n",
       "      <td>0.944142</td>\n",
       "      <td>0.958050</td>\n",
       "      <td>0.973953</td>\n",
       "      <td>0.953645</td>\n",
       "      <td>0.281453</td>\n",
       "      <td>0.170676</td>\n",
       "      <td>0.062751</td>\n",
       "      <td>0.048027</td>\n",
       "      <td>0.955417</td>\n",
       "      <td>0.984970</td>\n",
       "      <td>0.986171</td>\n",
       "      <td>0.965564</td>\n",
       "      <td>0.001</td>\n",
       "      <td>33591.138608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>30128</td>\n",
       "      <td>0.385774</td>\n",
       "      <td>0.189756</td>\n",
       "      <td>0.117316</td>\n",
       "      <td>0.078703</td>\n",
       "      <td>0.945609</td>\n",
       "      <td>0.959762</td>\n",
       "      <td>0.975018</td>\n",
       "      <td>0.955095</td>\n",
       "      <td>0.288143</td>\n",
       "      <td>0.169875</td>\n",
       "      <td>0.067963</td>\n",
       "      <td>0.050305</td>\n",
       "      <td>0.953274</td>\n",
       "      <td>0.983532</td>\n",
       "      <td>0.987004</td>\n",
       "      <td>0.961182</td>\n",
       "      <td>0.001</td>\n",
       "      <td>35828.734655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>32011</td>\n",
       "      <td>0.381422</td>\n",
       "      <td>0.186624</td>\n",
       "      <td>0.115684</td>\n",
       "      <td>0.079114</td>\n",
       "      <td>0.946209</td>\n",
       "      <td>0.959587</td>\n",
       "      <td>0.974729</td>\n",
       "      <td>0.955568</td>\n",
       "      <td>0.287478</td>\n",
       "      <td>0.171189</td>\n",
       "      <td>0.067297</td>\n",
       "      <td>0.048992</td>\n",
       "      <td>0.954712</td>\n",
       "      <td>0.983929</td>\n",
       "      <td>0.987698</td>\n",
       "      <td>0.964516</td>\n",
       "      <td>0.001</td>\n",
       "      <td>38064.828040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>33894</td>\n",
       "      <td>0.360983</td>\n",
       "      <td>0.174396</td>\n",
       "      <td>0.111832</td>\n",
       "      <td>0.074755</td>\n",
       "      <td>0.949008</td>\n",
       "      <td>0.961042</td>\n",
       "      <td>0.975896</td>\n",
       "      <td>0.957379</td>\n",
       "      <td>0.299115</td>\n",
       "      <td>0.173563</td>\n",
       "      <td>0.074466</td>\n",
       "      <td>0.051087</td>\n",
       "      <td>0.953472</td>\n",
       "      <td>0.979762</td>\n",
       "      <td>0.986855</td>\n",
       "      <td>0.963686</td>\n",
       "      <td>0.001</td>\n",
       "      <td>40301.608976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>35777</td>\n",
       "      <td>0.352864</td>\n",
       "      <td>0.169378</td>\n",
       "      <td>0.109918</td>\n",
       "      <td>0.073568</td>\n",
       "      <td>0.950574</td>\n",
       "      <td>0.961717</td>\n",
       "      <td>0.976139</td>\n",
       "      <td>0.959034</td>\n",
       "      <td>0.283525</td>\n",
       "      <td>0.170134</td>\n",
       "      <td>0.065517</td>\n",
       "      <td>0.047875</td>\n",
       "      <td>0.956399</td>\n",
       "      <td>0.983929</td>\n",
       "      <td>0.987351</td>\n",
       "      <td>0.965402</td>\n",
       "      <td>0.001</td>\n",
       "      <td>42540.375298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>37660</td>\n",
       "      <td>0.350967</td>\n",
       "      <td>0.167114</td>\n",
       "      <td>0.110551</td>\n",
       "      <td>0.073302</td>\n",
       "      <td>0.951209</td>\n",
       "      <td>0.961554</td>\n",
       "      <td>0.976337</td>\n",
       "      <td>0.959732</td>\n",
       "      <td>0.285932</td>\n",
       "      <td>0.174506</td>\n",
       "      <td>0.062480</td>\n",
       "      <td>0.048946</td>\n",
       "      <td>0.954613</td>\n",
       "      <td>0.984871</td>\n",
       "      <td>0.987847</td>\n",
       "      <td>0.964730</td>\n",
       "      <td>0.001</td>\n",
       "      <td>44779.249481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>39543</td>\n",
       "      <td>0.335067</td>\n",
       "      <td>0.159893</td>\n",
       "      <td>0.106344</td>\n",
       "      <td>0.068829</td>\n",
       "      <td>0.952821</td>\n",
       "      <td>0.962752</td>\n",
       "      <td>0.977445</td>\n",
       "      <td>0.961288</td>\n",
       "      <td>0.261067</td>\n",
       "      <td>0.157998</td>\n",
       "      <td>0.058686</td>\n",
       "      <td>0.044383</td>\n",
       "      <td>0.957847</td>\n",
       "      <td>0.986409</td>\n",
       "      <td>0.988244</td>\n",
       "      <td>0.968775</td>\n",
       "      <td>0.001</td>\n",
       "      <td>47018.757098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>41426</td>\n",
       "      <td>0.329203</td>\n",
       "      <td>0.155129</td>\n",
       "      <td>0.104919</td>\n",
       "      <td>0.069155</td>\n",
       "      <td>0.953877</td>\n",
       "      <td>0.963064</td>\n",
       "      <td>0.977566</td>\n",
       "      <td>0.961372</td>\n",
       "      <td>0.271106</td>\n",
       "      <td>0.167587</td>\n",
       "      <td>0.057825</td>\n",
       "      <td>0.045695</td>\n",
       "      <td>0.955605</td>\n",
       "      <td>0.986220</td>\n",
       "      <td>0.988194</td>\n",
       "      <td>0.966423</td>\n",
       "      <td>0.001</td>\n",
       "      <td>49257.533512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>43309</td>\n",
       "      <td>0.323371</td>\n",
       "      <td>0.153462</td>\n",
       "      <td>0.102698</td>\n",
       "      <td>0.067211</td>\n",
       "      <td>0.954704</td>\n",
       "      <td>0.964173</td>\n",
       "      <td>0.978204</td>\n",
       "      <td>0.961868</td>\n",
       "      <td>0.278785</td>\n",
       "      <td>0.168384</td>\n",
       "      <td>0.060851</td>\n",
       "      <td>0.049550</td>\n",
       "      <td>0.955407</td>\n",
       "      <td>0.984732</td>\n",
       "      <td>0.987302</td>\n",
       "      <td>0.965718</td>\n",
       "      <td>0.001</td>\n",
       "      <td>51498.481450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>45192</td>\n",
       "      <td>0.315833</td>\n",
       "      <td>0.147772</td>\n",
       "      <td>0.102191</td>\n",
       "      <td>0.065871</td>\n",
       "      <td>0.956230</td>\n",
       "      <td>0.963609</td>\n",
       "      <td>0.978441</td>\n",
       "      <td>0.962849</td>\n",
       "      <td>0.285185</td>\n",
       "      <td>0.168065</td>\n",
       "      <td>0.066611</td>\n",
       "      <td>0.050508</td>\n",
       "      <td>0.955675</td>\n",
       "      <td>0.983393</td>\n",
       "      <td>0.985814</td>\n",
       "      <td>0.962927</td>\n",
       "      <td>0.001</td>\n",
       "      <td>53738.328033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>47075</td>\n",
       "      <td>0.305535</td>\n",
       "      <td>0.143197</td>\n",
       "      <td>0.098557</td>\n",
       "      <td>0.063780</td>\n",
       "      <td>0.957451</td>\n",
       "      <td>0.964987</td>\n",
       "      <td>0.979204</td>\n",
       "      <td>0.964062</td>\n",
       "      <td>0.270033</td>\n",
       "      <td>0.165320</td>\n",
       "      <td>0.060070</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.956706</td>\n",
       "      <td>0.985565</td>\n",
       "      <td>0.988194</td>\n",
       "      <td>0.966900</td>\n",
       "      <td>0.001</td>\n",
       "      <td>55976.417146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>48958</td>\n",
       "      <td>0.305236</td>\n",
       "      <td>0.141225</td>\n",
       "      <td>0.100062</td>\n",
       "      <td>0.063949</td>\n",
       "      <td>0.958500</td>\n",
       "      <td>0.965116</td>\n",
       "      <td>0.978850</td>\n",
       "      <td>0.964570</td>\n",
       "      <td>0.271647</td>\n",
       "      <td>0.170199</td>\n",
       "      <td>0.057628</td>\n",
       "      <td>0.043821</td>\n",
       "      <td>0.956746</td>\n",
       "      <td>0.985565</td>\n",
       "      <td>0.988393</td>\n",
       "      <td>0.967589</td>\n",
       "      <td>0.001</td>\n",
       "      <td>58216.736588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>50841</td>\n",
       "      <td>0.295941</td>\n",
       "      <td>0.136409</td>\n",
       "      <td>0.097477</td>\n",
       "      <td>0.062055</td>\n",
       "      <td>0.959175</td>\n",
       "      <td>0.965129</td>\n",
       "      <td>0.979695</td>\n",
       "      <td>0.965751</td>\n",
       "      <td>0.291478</td>\n",
       "      <td>0.170358</td>\n",
       "      <td>0.072572</td>\n",
       "      <td>0.048548</td>\n",
       "      <td>0.955526</td>\n",
       "      <td>0.980903</td>\n",
       "      <td>0.986954</td>\n",
       "      <td>0.964648</td>\n",
       "      <td>0.001</td>\n",
       "      <td>60456.622695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>52724</td>\n",
       "      <td>0.293938</td>\n",
       "      <td>0.133499</td>\n",
       "      <td>0.097309</td>\n",
       "      <td>0.063131</td>\n",
       "      <td>0.960128</td>\n",
       "      <td>0.965391</td>\n",
       "      <td>0.979198</td>\n",
       "      <td>0.965527</td>\n",
       "      <td>0.284884</td>\n",
       "      <td>0.170615</td>\n",
       "      <td>0.067071</td>\n",
       "      <td>0.047198</td>\n",
       "      <td>0.956101</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>0.964684</td>\n",
       "      <td>0.001</td>\n",
       "      <td>62694.831374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>54607</td>\n",
       "      <td>0.291393</td>\n",
       "      <td>0.133663</td>\n",
       "      <td>0.097187</td>\n",
       "      <td>0.060543</td>\n",
       "      <td>0.960105</td>\n",
       "      <td>0.965157</td>\n",
       "      <td>0.980171</td>\n",
       "      <td>0.966094</td>\n",
       "      <td>0.275077</td>\n",
       "      <td>0.161095</td>\n",
       "      <td>0.064434</td>\n",
       "      <td>0.049548</td>\n",
       "      <td>0.958095</td>\n",
       "      <td>0.982540</td>\n",
       "      <td>0.987212</td>\n",
       "      <td>0.962086</td>\n",
       "      <td>0.001</td>\n",
       "      <td>64932.244809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>56490</td>\n",
       "      <td>0.282183</td>\n",
       "      <td>0.127107</td>\n",
       "      <td>0.095321</td>\n",
       "      <td>0.059756</td>\n",
       "      <td>0.961390</td>\n",
       "      <td>0.966625</td>\n",
       "      <td>0.980249</td>\n",
       "      <td>0.967007</td>\n",
       "      <td>0.278434</td>\n",
       "      <td>0.166009</td>\n",
       "      <td>0.064494</td>\n",
       "      <td>0.047930</td>\n",
       "      <td>0.956696</td>\n",
       "      <td>0.984276</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>0.964178</td>\n",
       "      <td>0.001</td>\n",
       "      <td>67171.251667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  iteration  train/loss  train/loss_grapheme  train/loss_vowel  \\\n",
       "0       1       1883    3.042697             1.797872          0.632122   \n",
       "1       2       3766    1.158309             0.645604          0.287593   \n",
       "2       3       5649    0.907438             0.502178          0.232452   \n",
       "3       4       7532    0.787359             0.430356          0.206906   \n",
       "4       5       9415    0.699629             0.377690          0.187407   \n",
       "5       6      11298    0.641063             0.340617          0.176144   \n",
       "6       7      13181    0.588314             0.309447          0.164080   \n",
       "7       8      15064    0.552341             0.288925          0.155283   \n",
       "8       9      16947    0.523410             0.270066          0.149229   \n",
       "9      10      18830    0.491826             0.250700          0.143343   \n",
       "10     11      20713    0.470240             0.238516          0.137160   \n",
       "11     12      22596    0.448688             0.226095          0.131934   \n",
       "12     13      24479    0.432559             0.214815          0.129309   \n",
       "13     14      26362    0.417307             0.207002          0.125265   \n",
       "14     15      28245    0.399159             0.196582          0.121286   \n",
       "15     16      30128    0.385774             0.189756          0.117316   \n",
       "16     17      32011    0.381422             0.186624          0.115684   \n",
       "17     18      33894    0.360983             0.174396          0.111832   \n",
       "18     19      35777    0.352864             0.169378          0.109918   \n",
       "19     20      37660    0.350967             0.167114          0.110551   \n",
       "20     21      39543    0.335067             0.159893          0.106344   \n",
       "21     22      41426    0.329203             0.155129          0.104919   \n",
       "22     23      43309    0.323371             0.153462          0.102698   \n",
       "23     24      45192    0.315833             0.147772          0.102191   \n",
       "24     25      47075    0.305535             0.143197          0.098557   \n",
       "25     26      48958    0.305236             0.141225          0.100062   \n",
       "26     27      50841    0.295941             0.136409          0.097477   \n",
       "27     28      52724    0.293938             0.133499          0.097309   \n",
       "28     29      54607    0.291393             0.133663          0.097187   \n",
       "29     30      56490    0.282183             0.127107          0.095321   \n",
       "\n",
       "    train/loss_consonant  train/acc_grapheme  train/acc_vowel  \\\n",
       "0               0.612704            0.601449         0.824641   \n",
       "1               0.225112            0.837768         0.912725   \n",
       "2               0.172807            0.871242         0.927178   \n",
       "3               0.150097            0.887900         0.933570   \n",
       "4               0.134532            0.900114         0.939088   \n",
       "5               0.124302            0.909256         0.941613   \n",
       "6               0.114787            0.916313         0.945563   \n",
       "7               0.108134            0.921472         0.948023   \n",
       "8               0.104115            0.925769         0.949912   \n",
       "9               0.097783            0.930137         0.951868   \n",
       "10              0.094564            0.933490         0.953323   \n",
       "11              0.090659            0.936000         0.955284   \n",
       "12              0.088435            0.939328         0.955757   \n",
       "13              0.085039            0.941782         0.957059   \n",
       "14              0.081291            0.944142         0.958050   \n",
       "15              0.078703            0.945609         0.959762   \n",
       "16              0.079114            0.946209         0.959587   \n",
       "17              0.074755            0.949008         0.961042   \n",
       "18              0.073568            0.950574         0.961717   \n",
       "19              0.073302            0.951209         0.961554   \n",
       "20              0.068829            0.952821         0.962752   \n",
       "21              0.069155            0.953877         0.963064   \n",
       "22              0.067211            0.954704         0.964173   \n",
       "23              0.065871            0.956230         0.963609   \n",
       "24              0.063780            0.957451         0.964987   \n",
       "25              0.063949            0.958500         0.965116   \n",
       "26              0.062055            0.959175         0.965129   \n",
       "27              0.063131            0.960128         0.965391   \n",
       "28              0.060543            0.960105         0.965157   \n",
       "29              0.059756            0.961390         0.966625   \n",
       "\n",
       "    train/acc_consonant  train/recall  valid/loss  valid/loss_grapheme  \\\n",
       "0              0.851282      0.677516    0.848359             0.480272   \n",
       "1              0.938502      0.873624    0.695029             0.378965   \n",
       "2              0.948852      0.898889    0.494653             0.275289   \n",
       "3              0.953656      0.912072    0.408498             0.241828   \n",
       "4              0.958386      0.920294    0.412999             0.236399   \n",
       "5              0.960942      0.927734    0.409563             0.246129   \n",
       "6              0.964307      0.933129    0.350197             0.194251   \n",
       "7              0.966103      0.937743    0.332787             0.198453   \n",
       "8              0.967451      0.940794    0.338975             0.199955   \n",
       "9              0.968854      0.944074    0.317771             0.193612   \n",
       "10             0.970448      0.946249    0.310357             0.178886   \n",
       "11             0.971813      0.948729    0.307876             0.175234   \n",
       "12             0.971614      0.950347    0.279739             0.161382   \n",
       "13             0.972991      0.952580    0.284997             0.165484   \n",
       "14             0.973953      0.953645    0.281453             0.170676   \n",
       "15             0.975018      0.955095    0.288143             0.169875   \n",
       "16             0.974729      0.955568    0.287478             0.171189   \n",
       "17             0.975896      0.957379    0.299115             0.173563   \n",
       "18             0.976139      0.959034    0.283525             0.170134   \n",
       "19             0.976337      0.959732    0.285932             0.174506   \n",
       "20             0.977445      0.961288    0.261067             0.157998   \n",
       "21             0.977566      0.961372    0.271106             0.167587   \n",
       "22             0.978204      0.961868    0.278785             0.168384   \n",
       "23             0.978441      0.962849    0.285185             0.168065   \n",
       "24             0.979204      0.964062    0.270033             0.165320   \n",
       "25             0.978850      0.964570    0.271647             0.170199   \n",
       "26             0.979695      0.965751    0.291478             0.170358   \n",
       "27             0.979198      0.965527    0.284884             0.170615   \n",
       "28             0.980171      0.966094    0.275077             0.161095   \n",
       "29             0.980249      0.967007    0.278434             0.166009   \n",
       "\n",
       "    valid/loss_vowel  valid/loss_consonant  valid/acc_grapheme  \\\n",
       "0           0.173948              0.194139            0.875774   \n",
       "1           0.182676              0.133387            0.896022   \n",
       "2           0.122158              0.097206            0.927242   \n",
       "3           0.092366              0.074304            0.935774   \n",
       "4           0.102951              0.073649            0.938056   \n",
       "5           0.092293              0.071141            0.934236   \n",
       "6           0.096681              0.059265            0.947093   \n",
       "7           0.075235              0.059099            0.946687   \n",
       "8           0.080003              0.059017            0.945744   \n",
       "9           0.070374              0.053785            0.948274   \n",
       "10          0.075267              0.056204            0.953175   \n",
       "11          0.078108              0.054535            0.953274   \n",
       "12          0.067744              0.050614            0.957202   \n",
       "13          0.069119              0.050394            0.956399   \n",
       "14          0.062751              0.048027            0.955417   \n",
       "15          0.067963              0.050305            0.953274   \n",
       "16          0.067297              0.048992            0.954712   \n",
       "17          0.074466              0.051087            0.953472   \n",
       "18          0.065517              0.047875            0.956399   \n",
       "19          0.062480              0.048946            0.954613   \n",
       "20          0.058686              0.044383            0.957847   \n",
       "21          0.057825              0.045695            0.955605   \n",
       "22          0.060851              0.049550            0.955407   \n",
       "23          0.066611              0.050508            0.955675   \n",
       "24          0.060070              0.044643            0.956706   \n",
       "25          0.057628              0.043821            0.956746   \n",
       "26          0.072572              0.048548            0.955526   \n",
       "27          0.067071              0.047198            0.956101   \n",
       "28          0.064434              0.049548            0.958095   \n",
       "29          0.064494              0.047930            0.956696   \n",
       "\n",
       "    valid/acc_vowel  valid/acc_consonant  valid/recall     lr  elapsed_time  \n",
       "0          0.960387             0.960526      0.909214  0.001   2243.661856  \n",
       "1          0.942252             0.966488      0.916853  0.001   4485.461718  \n",
       "2          0.966032             0.973413      0.939164  0.001   6727.550425  \n",
       "3          0.976637             0.979911      0.946010  0.001   8968.730490  \n",
       "4          0.972619             0.980060      0.951533  0.001  11209.000478  \n",
       "5          0.976091             0.979673      0.946735  0.001  13449.830865  \n",
       "6          0.972679             0.984028      0.950324  0.001  15688.977573  \n",
       "7          0.979911             0.982748      0.955584  0.001  17927.412009  \n",
       "8          0.978571             0.982540      0.958276  0.001  20167.192780  \n",
       "9          0.981300             0.985972      0.957762  0.001  22406.465798  \n",
       "10         0.980952             0.984673      0.960563  0.001  24645.514050  \n",
       "11         0.980159             0.985218      0.961931  0.001  26882.418484  \n",
       "12         0.983829             0.985565      0.965432  0.001  29116.622837  \n",
       "13         0.982391             0.985923      0.963083  0.001  31354.389972  \n",
       "14         0.984970             0.986171      0.965564  0.001  33591.138608  \n",
       "15         0.983532             0.987004      0.961182  0.001  35828.734655  \n",
       "16         0.983929             0.987698      0.964516  0.001  38064.828040  \n",
       "17         0.979762             0.986855      0.963686  0.001  40301.608976  \n",
       "18         0.983929             0.987351      0.965402  0.001  42540.375298  \n",
       "19         0.984871             0.987847      0.964730  0.001  44779.249481  \n",
       "20         0.986409             0.988244      0.968775  0.001  47018.757098  \n",
       "21         0.986220             0.988194      0.966423  0.001  49257.533512  \n",
       "22         0.984732             0.987302      0.965718  0.001  51498.481450  \n",
       "23         0.983393             0.985814      0.962927  0.001  53738.328033  \n",
       "24         0.985565             0.988194      0.966900  0.001  55976.417146  \n",
       "25         0.985565             0.988393      0.967589  0.001  58216.736588  \n",
       "26         0.980903             0.986954      0.964648  0.001  60456.622695  \n",
       "27         0.983333             0.987500      0.964684  0.001  62694.831374  \n",
       "28         0.982540             0.987212      0.962086  0.001  64932.244809  \n",
       "29         0.984276             0.987500      0.964178  0.001  67171.251667  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_history = log_report.get_dataframe()\n",
    "train_history.to_csv(outdir / 'log.csv', index=False)\n",
    "\n",
    "train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
